import requests
import re
import base64
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time

# æ ¸å¿ƒé…ç½®ï¼ˆæ˜ç¡®æ¯ä¸ªç½‘ç«™çš„YAMLè¾“å‡ºæ–‡ä»¶ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36"
}
# ç›®æ ‡ç½‘ç«™åˆ—è¡¨ï¼šç»‘å®šæ¯ä¸ªç½‘ç«™çš„åç§°ã€é¦–é¡µURLã€ä¸“å±YAMLè¾“å‡ºæ–‡ä»¶
TARGET_SITES = [
    {
        "name": "ç±³è´77",
        "url": "https://www.mibei77.com/",
        "yaml_file": "s2-clash.yaml"  # ç±³è´77çš„YAMLå•ç‹¬å­˜è¿™ä¸ªæ–‡ä»¶
    },
    {
        "name": "Datiya",
        "url": "https://free.datiya.com",
        "yaml_file": "s2-clash-2.yaml"  # Datiyaçš„YAMLå•ç‹¬å­˜è¿™ä¸ªæ–‡ä»¶
    }
]
NODE_OUTPUT_FILE = "s2.txt"  # æ‰€æœ‰ç½‘ç«™çš„txtèŠ‚ç‚¹åˆå¹¶åˆ°è¿™ä¸ªæ–‡ä»¶

def get_latest_article_url(site):
    """é€‚é…ä¸åŒç½‘ç«™ï¼Œè·å–æœ€æ–°çš„èŠ‚ç‚¹/è®¢é˜…æ–‡ç« é“¾æ¥"""
    site_name = site["name"]
    home_url = site["url"]
    print(f"\n========== å¼€å§‹å¤„ç† [{site_name}] ç½‘ç«™ ==========")
    
    try:
        resp = requests.get(home_url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        article_candidates = []
        today = datetime.now().strftime("%Y%m%d")
        yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")

        if site_name == "ç±³è´77":
            # é€‚é…mibei77çš„é“¾æ¥ç‰¹å¾ï¼šå«å½“æ—¥/æ˜¨æ—¥æ—¥æœŸçš„httpé“¾æ¥+å…³é”®è¯
            for a in soup.find_all('a', href=True):
                href = a['href']
                text = a.get_text(strip=True) + (a.get('title', ''))
                if ("èŠ‚ç‚¹" in text or "è®¢é˜…" in text or "å…è´¹" in text) and any(d in href for d in [today, yesterday]):
                    if href.startswith("http"):
                        article_candidates.append((href, text))
            # æŒ‰é“¾æ¥æ’åºå–æœ€æ–°
            if article_candidates:
                article_candidates.sort(key=lambda x: x[0], reverse=True)
                latest_url, latest_title = article_candidates[0]
                print(f"âœ… [{site_name}] æ‰¾åˆ°æœ€æ–°æ–‡ç« ï¼š{latest_title}")
                print(f"   é“¾æ¥ï¼š{latest_url}")
                return latest_url

        elif site_name == "Datiya":
            # é€‚é…datiyaçš„é“¾æ¥ç‰¹å¾ï¼š/post/æ—¥æœŸ/æ ¼å¼+å…³é”®è¯
            for a in soup.find_all('a', href=True):
                href = a['href']
                title = a.get_text(strip=True) or a.get('title', '')
                if ("èŠ‚ç‚¹" in title or "è®¢é˜…" in title or "å…è´¹" in title) and href.startswith("/post/"):
                    date_str = href.replace("/post/", "").strip("/")
                    try:
                        # åªç­›é€‰è¿‘2å¤©çš„æ–‡ç« 
                        article_date = datetime.strptime(date_str, "%Y%m%d")
                        if (datetime.now() - article_date).days <= 1:
                            full_url = f"{home_url}{href}"
                            article_candidates.append((article_date, full_url, title))
                    except:
                        continue
            # æŒ‰æ—¥æœŸæ’åºå–æœ€æ–°
            if article_candidates:
                article_candidates.sort(key=lambda x: x[0], reverse=True)
                _, latest_url, latest_title = article_candidates[0]
                print(f"âœ… [{site_name}] æ‰¾åˆ°æœ€æ–°æ–‡ç« ï¼š{latest_title}")
                print(f"   é“¾æ¥ï¼š{latest_url}")
                return latest_url

        print(f"âŒ [{site_name}] æœªæ‰¾åˆ°è¿‘2å¤©çš„æœ€æ–°æ–‡ç« ")
        return None

    except Exception as e:
        print(f"âŒ [{site_name}] è·å–é¦–é¡µå¤±è´¥ï¼š{e}")
        return None

def extract_sub_links(article_url, site_name):
    """ä»æ–‡ç« ä¸­æå–txtå’Œyamlé“¾æ¥ï¼Œè¿”å›(èŠ‚ç‚¹é“¾æ¥åˆ—è¡¨, YAMLé“¾æ¥åˆ—è¡¨)"""
    try:
        resp = requests.get(article_url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        text = resp.text
        soup = BeautifulSoup(text, 'html.parser')

        sub_links = set()  # .txtèŠ‚ç‚¹é“¾æ¥/Base64èŠ‚ç‚¹
        yaml_links = set() # .yamlé…ç½®é“¾æ¥
        exclude_domains = ["reddit", "telegram", "twitter", "facebook"]

        # 1. æå–æ‰€æœ‰.txté“¾æ¥ï¼ˆå«èŠ‚ç‚¹/è®¢é˜…å…³é”®è¯ï¼‰
        txt_links = re.findall(r'https?://[^\s<>"\']*\.txt', text)
        for link in txt_links:
            if any(k in link.lower() for k in ["sub", "node", "v2ray", "clash", "bagtr"]):
                sub_links.add(link)

        # 2. æå–æ‰€æœ‰.yamlé“¾æ¥ï¼ˆå«clashå…³é”®è¯ï¼‰
        yaml_links_raw = re.findall(r'https?://[^\s<>"\']*\.yaml', text)
        for link in yaml_links_raw:
            if "clash" in link.lower():
                yaml_links.add(link)

        # 3. æå–æ–‡ç« å†…çš„Base64æ ¼å¼èŠ‚ç‚¹ï¼ˆç›´æ¥å†™çš„é•¿å­—ç¬¦ä¸²ï¼‰
        for tag in soup.find_all(['pre', 'code', 'p', 'div']):
            parts = re.split(r'\s+', tag.get_text())
            for part in parts:
                part = part.strip()
                if len(part) > 100 and re.match(r'^[A-Za-z0-9+/=]+$', part):
                    try:
                        base64.b64decode(part, validate=True)
                        sub_links.add(part)
                    except:
                        pass

        # è¿‡æ»¤ç¤¾äº¤ç±»æ— ç”¨é“¾æ¥
        sub_links = {l for l in sub_links if not any(ex in l for ex in exclude_domains)}
        yaml_links = {l for l in yaml_links if not any(ex in l for ex in exclude_domains)}

        # æ—¥å¿—è¾“å‡ºæå–ç»“æœï¼ˆæ¸…æ™°å±•ç¤ºæ•°é‡å’Œå…·ä½“é“¾æ¥ï¼‰
        print(f"\nğŸ“Œ [{site_name}] æå–ç»“æœï¼š")
        print(f"   - æœ‰æ•ˆ.txt/Base64èŠ‚ç‚¹æºï¼š{len(sub_links)} ä¸ª")
        for i, l in enumerate(sorted(sub_links), 1):
            print(f"     {i}. {l[:70]}..." if len(l) > 70 else f"     {i}. {l}")
        print(f"   - æœ‰æ•ˆ.yamlé…ç½®æºï¼š{len(yaml_links)} ä¸ª")
        for i, l in enumerate(sorted(yaml_links), 1):
            print(f"     {i}. {l[:70]}..." if len(l) > 70 else f"     {i}. {l}")

        return list(sub_links), list(yaml_links)

    except Exception as e:
        print(f"âŒ [{site_name}] è§£ææ–‡ç« å¤±è´¥ï¼š{e}")
        return [], []

def download_nodes(source):
    """ä¸‹è½½å•ä¸ªtxt/Base64æºï¼Œè¿”å›èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå«è¯¦ç»†æ—¥å¿—ï¼‰"""
    try:
        # åŒºåˆ†æ˜¯URLé“¾æ¥è¿˜æ˜¯Base64å­—ç¬¦ä¸²
        if source.startswith('http'):
            resp = requests.get(source, headers=HEADERS, timeout=20)
            resp.raise_for_status()
            raw_content = resp.text.strip()
        else:
            raw_content = base64.b64decode(source).decode('utf-8', errors='ignore').strip()

        nodes = []
        # æå–æ˜æ–‡èŠ‚ç‚¹ï¼ˆè¦†ç›–ä¸»æµåè®®ï¼‰
        for line in raw_content.split('\n'):
            line = line.strip()
            if line.startswith(('vmess://', 'vless://', 'trojan://', 'ss://', 'hysteria://')):
                nodes.append(line)

        # è‹¥æ²¡æœ‰æ˜æ–‡èŠ‚ç‚¹ï¼Œå°è¯•è§£ææ¯è¡Œçš„Base64ç¼–ç èŠ‚ç‚¹ï¼ˆéƒ¨åˆ†ç½‘ç«™çš„å­˜å‚¨æ–¹å¼ï¼‰
        if not nodes:
            for line in raw_content.split('\n'):
                line = line.strip()
                if len(line) > 50 and re.match(r'^[A-Za-z0-9+/=]+$', line):
                    try:
                        decoded = base64.b64decode(line, validate=True).decode('utf-8', errors='ignore')
                        for sub_line in decoded.split('\n'):
                            sub_line = sub_line.strip()
                            if sub_line.startswith(('vmess://', 'vless://', 'trojan://')):
                                nodes.append(sub_line)
                    except:
                        continue

        print(f"   âœ¨ ä» [{source[:50]}...] æå–åˆ° {len(nodes)} ä¸ªæœ‰æ•ˆèŠ‚ç‚¹")
        return nodes

    except Exception as e:
        print(f"   âŒ å¤„ç† [{source[:50]}...] å¤±è´¥ï¼š{e}")
        return []

def download_and_save_yaml(yaml_url, output_file, site_name):
    """ä¸‹è½½æŒ‡å®šYAMLé“¾æ¥å¹¶ä¿å­˜åˆ°ä¸“å±æ–‡ä»¶ï¼Œè¿”å›æ˜¯å¦æˆåŠŸ"""
    try:
        resp = requests.get(yaml_url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        yaml_content = resp.text.strip()
        # ä¿å­˜åˆ°è¯¥ç½‘ç«™ä¸“å±çš„YAMLæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(yaml_content)
        print(f"âœ… [{site_name}] YAMLé…ç½®å·²ä¿å­˜åˆ°ï¼š{output_file}")
        return True
    except Exception as e:
        print(f"âŒ [{site_name}] ä¸‹è½½YAMLå¤±è´¥ [{yaml_url[:70]}...]ï¼š{e}")
        return False

def main():
    print("========== å¼€å§‹æŠ“å–ä¸¤ä¸ªç½‘ç«™çš„æœ€æ–°èµ„æº ==========")
    all_nodes = []  # å­˜å‚¨æ‰€æœ‰ç½‘ç«™çš„èŠ‚ç‚¹ï¼ˆå»é‡å‰ï¼‰

    # éå†æ¯ä¸ªç›®æ ‡ç½‘ç«™ï¼Œé€ä¸ªå¤„ç†
    for site in TARGET_SITES:
        site_name = site["name"]
        yaml_output = site["yaml_file"]
        
        # 1. è·å–è¯¥ç½‘ç«™æœ€æ–°æ–‡ç« é“¾æ¥
        article_url = get_latest_article_url(site)
        if not article_url:
            continue
        
        # 2. ä»æ–‡ç« ä¸­æå–txtèŠ‚ç‚¹æºå’Œyamlé…ç½®é“¾æ¥
        sub_links, yaml_links = extract_sub_links(article_url, site_name)
        
        # 3. ä¸‹è½½å¹¶æ”¶é›†è¯¥ç½‘ç«™çš„txtèŠ‚ç‚¹ï¼ˆç”¨äºåç»­åˆå¹¶ï¼‰
        if sub_links:
            print(f"\nğŸ“¥ [{site_name}] å¼€å§‹ä¸‹è½½èŠ‚ç‚¹æºï¼š")
            for src in sub_links:
                nodes = download_nodes(src)
                all_nodes.extend(nodes)
                time.sleep(0.5)  # é˜²è¯·æ±‚è¿‡å¿«è¢«æ‹¦æˆª
        
        # 4. ä¸‹è½½å¹¶ä¿å­˜è¯¥ç½‘ç«™çš„YAMLé…ç½®ï¼ˆä¼˜å…ˆç¬¬ä¸€ä¸ªå¯ç”¨çš„ï¼‰
        if yaml_links:
            print(f"\nğŸ“¥ [{site_name}] å¼€å§‹ä¸‹è½½YAMLé…ç½®ï¼š")
            yaml_downloaded = False
            for yaml_url in yaml_links:
                if download_and_save_yaml(yaml_url, yaml_output, site_name):
                    yaml_downloaded = True
                    break
            if not yaml_downloaded:
                print(f"âŒ [{site_name}] æ‰€æœ‰YAMLé“¾æ¥ä¸‹è½½å¤±è´¥ï¼Œ{yaml_output} æœªç”Ÿæˆ")
        else:
            print(f"\nâŒ [{site_name}] æœªæ‰¾åˆ°ä»»ä½•YAMLé…ç½®é“¾æ¥ï¼Œ{yaml_output} æœªç”Ÿæˆ")

    # ========== å¤„ç†æ‰€æœ‰ç½‘ç«™çš„èŠ‚ç‚¹ï¼šåˆå¹¶ã€å»é‡ã€ä¿å­˜ ==========
    print("\n========== æ‰€æœ‰èŠ‚ç‚¹åˆå¹¶å»é‡ ==========")
    # èŠ‚ç‚¹å»é‡ï¼ˆä¿æŒé¡ºåºï¼Œé¿å…é‡å¤ï¼‰
    seen_nodes = set()
    unique_nodes = []
    for node in all_nodes:
        if node not in seen_nodes:
            seen_nodes.add(node)
            unique_nodes.append(node)
    
    # ä¿å­˜åˆå¹¶åçš„èŠ‚ç‚¹åˆ°s2.txtï¼ˆBase64ç¼–ç ï¼Œå…¼å®¹å®¢æˆ·ç«¯å¯¼å…¥ï¼‰
    if unique_nodes:
        node_content = '\n'.join(unique_nodes)
        encoded_content = base64.b64encode(node_content.encode('utf-8')).decode('utf-8')
        with open(NODE_OUTPUT_FILE, 'w', encoding='utf-8') as f:
            f.write(encoded_content)
        print(f"âœ… ä¸¤ä¸ªç½‘ç«™èŠ‚ç‚¹åˆå¹¶å®Œæˆï¼šå…± {len(unique_nodes)} ä¸ªå”¯ä¸€èŠ‚ç‚¹")
        print(f"   å·²ä¿å­˜åˆ° {NODE_OUTPUT_FILE}ï¼ˆBase64ç¼–ç ï¼‰")
    else:
        print(f"âŒ æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆèŠ‚ç‚¹ï¼Œ{NODE_OUTPUT_FILE} æœªç”Ÿæˆ")

    print("\n========== æŠ“å–ä»»åŠ¡å…¨éƒ¨å®Œæˆ ==========")

if __name__ == "__main__":
    main()
